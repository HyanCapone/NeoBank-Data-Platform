# NeoBank Data Platform

## ğŸ“Œ VisÃ£o Geral
Projeto de Engenharia de Dados simulando o ambiente de um NeoBank. O objetivo Ã© construir uma plataforma de dados moderna na nuvem (Azure), aplicando conceitos de Big Data, processamento distribuÃ­do (Apache Spark via Databricks) e Data Warehousing (Azure Synapse Analytics).

## ğŸ›ï¸ Arquitetura (Medallion Architecture)

Utilizamos a arquitetura medalhÃ£o para garantir a qualidade e governanÃ§a dos dados em diferentes estÃ¡gios de refinamento.

ğŸ‘‰ ğŸ’¸ **FinOps & Escalabilidade:** Desenvolvido com foco em Custo-EficiÃªncia e limites corporativos (Enterprise). Confira nosso [EsboÃ§o de Estimativa de Custos e Escalabilidade](docs/cost_estimation_and_scale.md) para entender como essa POC roda por **$8.54/mÃªs**, e exatamente quando a arquitetura deve escalar suas unidades de computaÃ§Ã£o (DWUs e Spark Workers) para volumes de dados reais em ProduÃ§Ã£o mÃ­nima.

```mermaid
graph TD
    subgraph Fontes de Dados
    A[Sistemas Transacionais / CSV]
    end

    subgraph Azure Data Lake Storage Gen2
    B[(Raw / Landing zone)]
    C[(Bronze / Raw Data)]
    D[(Silver / Cleansed & SCD2)]
    E[(Gold / Aggregations)]
    end

    subgraph Processamento
    P1{Azure Databricks <br/> PySpark}
    end

    subgraph Analytics & Serving
    F[Azure Synapse Analytics <br/> Serverless SQL Pool]
    G[Power BI]
    end

    A -->|IngestÃ£o| B
    B -->|Leitura RAW| P1
    P1 -->|ValidaÃ§Ã£o e Schema| C
    C -->|TransformaÃ§Ã£o e Filtro| P1
    P1 -->|Slowly Changing Dimension Type 2| D
    D -->|AgregaÃ§Ãµes de NegÃ³cio| P1
    P1 -->|Carga Gold| E
    
    E -->|Polybase / Tabelas Externas| F
    F -->|DirectQuery / Import| G
```

## ğŸ“‚ Estrutura do RepositÃ³rio

Organizamos o cÃ³digo simulando um ambiente produtivo real:

```text
â”œâ”€â”€ docs/                  # DocumentaÃ§Ã£o de arquitetura, dicionÃ¡rio de dados e decisÃµes (ADRs)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ notebooks/         # Notebooks exportados do Databricks (.ipynb / .py)
â”‚   â”œâ”€â”€ sql/               # Scripts DDL/DML, criaÃ§Ã£o de External Tables e Views no Synapse
â”‚   â””â”€â”€ pipelines/         # Arquivos de orquestraÃ§Ã£o 
â”œâ”€â”€ .gitignore             # Arquivos a serem ignorados pelo Git (.csv pesados, credenciais, etc)
â””â”€â”€ README.md              # Este arquivo!
```

## ğŸš€ Tecnologias e SoluÃ§Ãµes TÃ©cnicas
- **Armazenamento:** Azure Data Lake Storage Gen2 (Arquitetura MedalhÃ£o: Bronze, Silver, Gold).
- **Processamento ELT:** Azure Databricks (Apache Spark / PySpark).
- **Data Warehouse:** Azure Synapse Analytics (Serverless SQL Pool, Dedicated SQL Pool, PolyBase).
- **Modelagem AnalÃ­tica:** ImplementaÃ§Ã£o nativa de **Slowly Changing Dimensions (SCD Tipo 2)** no PySpark, preservando o histÃ³rico de dados na camada Silver.
- **DeduplicaÃ§Ã£o Inteligente:** Uso de Window Functions (`row_number()`) e ordenaÃ§Ã£o para ingestÃ£o da versÃ£o ativa do registro.
- **Performance de Consulta:** Tabela final provisionada com **Massively Parallel Processing (MPP)** usando `DISTRIBUTION = HASH` e indexaÃ§Ã£o `CLUSTERED COLUMNSTORE INDEX` para Analytics.
- **SeguranÃ§a (Estado da Arte):** **Unity Catalog com External Locations** (via Access Connector for Azure Databricks / Managed Identities). Nenhuma senha ou SAS Token fixo trafega no cÃ³digo PySpark!
